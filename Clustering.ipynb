{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "current_palette = sns.color_palette()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from IPython.display import clear_output\n",
    "sns.set_style('darkgrid')\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import calinski_harabasz_score, silhouette_score\n",
    "import retrieve as rt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import obtain as ob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty_df = ob.upload_county_acs_data('ACSDP5Y2012.DP03_data_with_overlays_2019-12-31T163946.csv')\n",
    "df = poverty_df\n",
    "population_df = ob.upload_county_acs_data('ACSDP5Y2012.DP05_data_with_overlays_2019-12-31T193014.csv')\n",
    "df = ob.merge_acs_data(df,population_df)\n",
    "df = ob.remove_duplicate_countystate(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean2, object_df = ob.separate_num_columns(df)\n",
    "#clean = clean.drop(columns= ['ratio_to_max_payment', '_Average_Total_Payments_'])\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(clean2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1ed6e52dcc78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Run the clustering algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mk_means\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Generate cluster index values for each row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\learn-env\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    970\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 972\u001b[1;33m                 return_n_iter=True)\n\u001b[0m\u001b[0;32m    973\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\learn-env\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[1;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy_x\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n\u001b[1;32m--> 312\u001b[1;33m                     order=order, copy=copy_x)\n\u001b[0m\u001b[0;32m    313\u001b[0m     \u001b[1;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "cluster = list(range(2,30))\n",
    "calinski_harabasz_list = []\n",
    "\n",
    "for i in cluster:\n",
    "    # Set number of clusters at initialization time\n",
    "    k_means = KMeans(n_clusters=i)\n",
    "\n",
    "    # Run the clustering algorithm\n",
    "    k_means.fit(scaled_df) \n",
    "\n",
    "    # Generate cluster index values for each row\n",
    "    cluster_assignments = k_means.predict(scaled_df) \n",
    "    \n",
    "    transfer_dict = {'Score':calinski_harabasz_score(scaled_df, cluster_assignments), 'Cluster':i}\n",
    "    calinski_harabasz_list.append(transfer_dict)\n",
    "    \n",
    "    print(transfer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(calinski_harabasz_list).set_index('Cluster').plot()\n",
    "plt.xticks(range(2,30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last = calinski_harabasz_list[0]['Score']\n",
    "for i in calinski_harabasz_list[1:]:\n",
    "    plt.scatter(i['Cluster'],(i['Score']- last))\n",
    "    last = i['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms_df = ob.cms_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = ob.cms_procedure_dummy_labels(cms_df)\n",
    "for i in label_df.columns:\n",
    "    label_df[i] = label_df[i]* cms_df.ratio_to_max_payment\n",
    "label_df['Provider_Id'] = cms_df['Provider_Id']\n",
    "label_df = label_df.groupby('Provider_Id').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df, object_df = ob.separate_num_columns(label_df)\n",
    "clean = num_df\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cluster = list(range(2,30))\n",
    "calinski_harabasz_list = []\n",
    "\n",
    "for i in cluster:\n",
    "    # Set number of clusters at initialization time\n",
    "    k_means = KMeans(n_clusters=i)\n",
    "\n",
    "    # Run the clustering algorithm\n",
    "    k_means.fit(scaled_df) \n",
    "\n",
    "    # Generate cluster index values for each row\n",
    "    cluster_assignments = k_means.predict(scaled_df) \n",
    "    \n",
    "    transfer_dict = {'Score':calinski_harabasz_score(scaled_df, cluster_assignments), 'Cluster':i}\n",
    "    calinski_harabasz_list.append(transfer_dict)\n",
    "    \n",
    "    print(transfer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(calinski_harabasz_list).set_index('Cluster').plot()\n",
    "plt.xticks(range(2,30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last = calinski_harabasz_list[0]['Score']\n",
    "velocity =[0]\n",
    "for i in calinski_harabasz_list[1:]:\n",
    "    plt.scatter(i['Cluster'],(i['Score']- last))\n",
    "    velocity.append(i['Score']- last)\n",
    "    last = i['Score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cms_df.merge(df.drop(columns =['County','State']), how='left', on=['CountyState'])\n",
    "print(len(df))\n",
    "label_df = df.groupby('Provider_Id').mean()\n",
    "num_df, object_df = ob.separate_num_columns(label_df)\n",
    "clean1 = num_df\n",
    "print(len(num_df))\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(clean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cluster = list(range(2,30))\n",
    "calinski_harabasz_list = []\n",
    "\n",
    "for i in cluster:\n",
    "    # Set number of clusters at initialization time\n",
    "    k_means = KMeans(n_clusters=i)\n",
    "\n",
    "    # Run the clustering algorithm\n",
    "    k_means.fit(scaled_df) \n",
    " \n",
    "    # Generate cluster index values for each row\n",
    "    cluster_assignments = k_means.predict(scaled_df) \n",
    "    \n",
    "    transfer_dict = {'Score':calinski_harabasz_score(scaled_df, cluster_assignments), 'Cluster':i}\n",
    "    calinski_harabasz_list.append(transfer_dict)\n",
    "    \n",
    "    print(transfer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(calinski_harabasz_list).set_index('Cluster').plot()\n",
    "plt.xticks(range(2,30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last = calinski_harabasz_list[0]['Score']\n",
    "for i in calinski_harabasz_list[1:]:\n",
    "    plt.scatter(i['Cluster'],(i['Score']- last))\n",
    "    last = i['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters=10)\n",
    "\n",
    "# Run the clustering algorithm\n",
    "k_means.fit(scaled_df) \n",
    "\n",
    "# Generate cluster index values for each row\n",
    "cluster_assignments = k_means.predict(scaled_df)\n",
    "sho= label_df\n",
    "sho['Cluster'] = cluster_assignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in sho['Cluster'].unique():\n",
    "    sns.distplot(sho.loc[sho['Cluster'] == i]['SEX_AND_AGE_Total_population'])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in sho['Cluster'].unique():\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.distplot(sho.loc[sho['Cluster'] == i]['RACE_One_race_Asian_Chinese'])\n",
    "    plt.xlim(0,sho.loc[sho['Cluster'] == i]['RACE_One_race_Asian_Chinese'].max())\n",
    "    plt.show()\n",
    "    print(sho.loc[sho['Cluster'] == i]['RACE_One_race_Asian_Chinese'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2 =df\n",
    "show2['Cluster'] = df.Provider_Id.apply(lambda x: sho['Cluster'][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column= 'ratio_to_max_payment'\n",
    "for i in show2['Cluster'].unique():\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.distplot(show2.loc[show2['Cluster'] == i][column])\n",
    "    plt.xlim(0,show2.loc[show2['Cluster'] == i][column].max())\n",
    "    plt.show()\n",
    "    print(show2.loc[show2['Cluster'] == i][column].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-learn-env] *",
   "language": "python",
   "name": "conda-env-.conda-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
